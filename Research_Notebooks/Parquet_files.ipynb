{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet Files\n",
    "Parquet is a columnar storage file format designed for efficient processing and storage in big data environments. It stores data column-wise, allowing for better compression and performance. Key features include support for various compression algorithms, schema evolution, cross-language compatibility, and optimized performance with big data processing frameworks like Apache Spark and Apache Hive. Parquet is commonly used in data lakes and warehouses due to its efficiency and flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame:\n",
      "       host_name db_name table_name  rows_read  rows_inserted  rows_deleted  \\\n",
      "0         host_1    db_1      tbl_1        142            302            16   \n",
      "1         host_1    db_1      tbl_2        560            623           592   \n",
      "2         host_1    db_1      tbl_3          9            739            22   \n",
      "3         host_1    db_1      tbl_4        189             23           256   \n",
      "4         host_1    db_1      tbl_5        824            227           563   \n",
      "...          ...     ...        ...        ...            ...           ...   \n",
      "806395    host_1    db_1    tbl_196        760             55           936   \n",
      "806396    host_1    db_1    tbl_197          4            147           957   \n",
      "806397    host_1    db_1    tbl_198         75            669           271   \n",
      "806398    host_1    db_1    tbl_199        360            554           842   \n",
      "806399    host_1    db_1    tbl_200        425             51           840   \n",
      "\n",
      "        rows_updated         insert_date  \n",
      "0                429 2023-10-01 10:00:00  \n",
      "1                243 2023-10-01 10:00:00  \n",
      "2                326 2023-10-01 10:00:00  \n",
      "3                427 2023-10-01 10:00:00  \n",
      "4                309 2023-10-01 10:00:00  \n",
      "...              ...                 ...  \n",
      "806395           494 2023-10-15 09:55:00  \n",
      "806396           217 2023-10-15 09:55:00  \n",
      "806397           415 2023-10-15 09:55:00  \n",
      "806398           563 2023-10-15 09:55:00  \n",
      "806399           804 2023-10-15 09:55:00  \n",
      "\n",
      "[806400 rows x 8 columns]\n",
      "\n",
      "DataFrame saved to output_data.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Function to generate random data with constant host name, database, and timestamp\n",
    "def generate_batch_data(batch_size, batch_number, base_insert_date):\n",
    "    data = {\n",
    "        'host_name': ['host_1'] * batch_size,\n",
    "        'db_name': ['db_1'] * batch_size,\n",
    "        'table_name': [f'tbl_{i + 1}' for i in range(batch_size)],\n",
    "        'rows_read': [random.randint(1, 1000) for _ in range(batch_size)],\n",
    "        'rows_inserted': [random.randint(1, 1000) for _ in range(batch_size)],\n",
    "        'rows_deleted': [random.randint(1, 1000) for _ in range(batch_size)],\n",
    "        'rows_updated': [random.randint(1, 1000) for _ in range(batch_size)],\n",
    "        'insert_date': [base_insert_date + timedelta(minutes=5 * batch_number) for _ in range(batch_size)],\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Number of batches and batch size\n",
    "num_batches = 12 * 24 * 14\n",
    "batch_size = 200\n",
    "\n",
    "# Base insert date for the first batch\n",
    "base_insert_date = datetime(2023, 10, 1, 10, 0, 0)\n",
    "\n",
    "# List to store DataFrames for each batch\n",
    "dfs = []\n",
    "\n",
    "# Generate and insert data in batches\n",
    "for i in range(num_batches):\n",
    "    df = generate_batch_data(batch_size, i, base_insert_date)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all batches into a single DataFrame\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(\"DataFrame:\")\n",
    "print(final_df)\n",
    "\n",
    "# Save the DataFrame to a Parquet file\n",
    "parquet_file_path = 'output_data.parquet'\n",
    "final_df.to_parquet(parquet_file_path, index=False)\n",
    "\n",
    "print(f\"\\nDataFrame saved to {parquet_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of DataFrame: 174.93 MB\n",
      "Size of Parquet File on OS: 3.95 MB\n",
      "Difference (DataFrame - Parquet): 170.97 MB\n",
      "Percentage Difference (The Parquet as a percentage of the dataframe): 2.26%\n",
      "\n",
      "DataFrame saved to output_data.parquet\n"
     ]
    }
   ],
   "source": [
    "# Print the size of the DataFrame\n",
    "dataframe_size_mb = final_df.memory_usage(index=False, deep=True).sum() / (1024 * 1024)\n",
    "print(f\"Size of DataFrame: {dataframe_size_mb:.2f} MB\")\n",
    "\n",
    "# Print the size of the Parquet file\n",
    "parquet_size_mb = os.path.getsize(parquet_file_path) / (1024 * 1024)\n",
    "print(f\"Size of Parquet File on OS: {parquet_size_mb:.2f} MB\")\n",
    "\n",
    "# Print the difference between the two sizes\n",
    "size_diff = dataframe_size_mb - parquet_size_mb\n",
    "print(f\"Difference (DataFrame - Parquet): {size_diff:.2f} MB\")\n",
    "\n",
    "# Calculate and print the percentage difference\n",
    "percentage_diff = (size_diff / dataframe_size_mb) * 100\n",
    "print(f\"Percentage Difference (The Parquet as a percentage of the dataframe): {100 - percentage_diff:.2f}%\")\n",
    "\n",
    "\n",
    "print(f\"\\nDataFrame saved to {parquet_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Parquet file has 806400 rows.\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Parquet file path\n",
    "parquet_file_path = 'output_data.parquet'\n",
    "\n",
    "# Read the Parquet file\n",
    "parquet_table = pq.read_table(parquet_file_path)\n",
    "\n",
    "# Get the number of rows\n",
    "num_rows = parquet_table.num_rows\n",
    "\n",
    "# Print the number of rows\n",
    "print(f\"The Parquet file has {num_rows} rows.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series opetations. \n",
    "Notice the file of the file (4 measures, history of 14 days, every 5 min ) is less than 4 MB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   hour  max_rows_read  avg_rows_read\n",
      "0   2023-10-01 10:00:00           1000     509.561250\n",
      "1   2023-10-01 11:00:00           1000     503.667500\n",
      "2   2023-10-01 12:00:00           1000     499.016667\n",
      "3   2023-10-01 13:00:00           1000     502.116250\n",
      "4   2023-10-01 14:00:00           1000     498.920000\n",
      "..                  ...            ...            ...\n",
      "331 2023-10-15 05:00:00           1000     499.235417\n",
      "332 2023-10-15 06:00:00           1000     503.304583\n",
      "333 2023-10-15 07:00:00           1000     497.774583\n",
      "334 2023-10-15 08:00:00            999     499.756250\n",
      "335 2023-10-15 09:00:00           1000     502.877500\n",
      "\n",
      "[336 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Parquet file path\n",
    "parquet_file_path = 'output_data.parquet'\n",
    "\n",
    "# Read the Parquet file\n",
    "parquet_table = pq.read_table(parquet_file_path)\n",
    "\n",
    "# Convert the Parquet table to a DataFrame\n",
    "df = parquet_table.to_pandas()\n",
    "\n",
    "# Convert 'insert_date' to datetime type\n",
    "df['insert_date'] = pd.to_datetime(df['insert_date'])\n",
    "\n",
    "# Group by hourly time buckets and calculate max and average of 'rows_read'\n",
    "result_df = df.groupby(pd.Grouper(key='insert_date', freq='H')).agg({\n",
    "    'rows_read': ['max', 'mean']\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "result_df.columns = ['hour', 'max_rows_read', 'avg_rows_read']\n",
    "\n",
    "# Print the result\n",
    "print(result_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
