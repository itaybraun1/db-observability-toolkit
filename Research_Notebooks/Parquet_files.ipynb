{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet Files\n",
    "Parquet is a columnar storage file format designed for efficient processing and storage in big data environments. It stores data column-wise, allowing for better compression and performance. Key features include support for various compression algorithms, schema evolution, cross-language compatibility, and optimized performance with big data processing frameworks like Apache Spark and Apache Hive. Parquet is commonly used in data lakes and warehouses due to its efficiency and flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame:\n",
      "        host_name db_name table_name  rows_read  rows_inserted  rows_deleted  \\\n",
      "0          host_1    db_1      tbl_1     266903            891           118   \n",
      "1          host_1    db_1      tbl_2     438303            307           478   \n",
      "2          host_1    db_1      tbl_3     448253            117           869   \n",
      "3          host_1    db_1      tbl_4     488629            310           265   \n",
      "4          host_1    db_1      tbl_5     271802            871           150   \n",
      "...           ...     ...        ...        ...            ...           ...   \n",
      "1209595    host_1    db_1    tbl_296     222398            792           327   \n",
      "1209596    host_1    db_1    tbl_297     301202            844           987   \n",
      "1209597    host_1    db_1    tbl_298     543543            939            50   \n",
      "1209598    host_1    db_1    tbl_299     324752            239           559   \n",
      "1209599    host_1    db_1    tbl_300     657417            374           960   \n",
      "\n",
      "         rows_updated         insert_date  \n",
      "0                 629 2023-10-01 10:00:00  \n",
      "1                 557 2023-10-01 10:00:00  \n",
      "2                 747 2023-10-01 10:00:00  \n",
      "3                 271 2023-10-01 10:00:00  \n",
      "4                 452 2023-10-01 10:00:00  \n",
      "...               ...                 ...  \n",
      "1209595           192 2023-10-15 09:55:00  \n",
      "1209596           771 2023-10-15 09:55:00  \n",
      "1209597           623 2023-10-15 09:55:00  \n",
      "1209598           177 2023-10-15 09:55:00  \n",
      "1209599            39 2023-10-15 09:55:00  \n",
      "\n",
      "[1209600 rows x 8 columns]\n",
      "\n",
      "DataFrame saved to output_data.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Function to generate random data with constant host name, database, and timestamp\n",
    "def generate_batch_data(batch_size, batch_number, base_insert_date):\n",
    "    data = {\n",
    "        'host_name': ['host_1'] * batch_size,\n",
    "        'db_name': ['db_1'] * batch_size,\n",
    "        'table_name': [f'tbl_{i + 1}' for i in range(batch_size)],\n",
    "        'rows_read': [random.randint(1, 1000000) for _ in range(batch_size)],\n",
    "        'rows_inserted': [random.randint(1, 1000) for _ in range(batch_size)],\n",
    "        'rows_deleted': [random.randint(1, 1000) for _ in range(batch_size)],\n",
    "        'rows_updated': [random.randint(1, 1000) for _ in range(batch_size)],\n",
    "        'insert_date': [base_insert_date + timedelta(minutes=5 * batch_number) for _ in range(batch_size)],\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Number of batches and batch size\n",
    "num_batches = 12 * 24 * 14\n",
    "batch_size = 300\n",
    "\n",
    "# Base insert date for the first batch\n",
    "base_insert_date = datetime(2023, 10, 1, 10, 0, 0)\n",
    "\n",
    "# List to store DataFrames for each batch\n",
    "dfs = []\n",
    "\n",
    "# Generate and insert data in batches\n",
    "for i in range(num_batches):\n",
    "    df = generate_batch_data(batch_size, i, base_insert_date)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all batches into a single DataFrame\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(\"DataFrame:\")\n",
    "print(final_df)\n",
    "\n",
    "# Save the DataFrame to a Parquet file\n",
    "parquet_file_path = 'output_data.parquet'\n",
    "final_df.to_parquet(parquet_file_path, index=False)\n",
    "\n",
    "print(f\"\\nDataFrame saved to {parquet_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of DataFrame: 262.60 MB\n",
      "Size of Parquet File on OS: 10.42 MB\n",
      "Difference (DataFrame - Parquet): 252.18 MB\n",
      "Percentage Difference (The Parquet as a percentage of the dataframe): 3.97%\n",
      "\n",
      "DataFrame saved to output_data.parquet\n"
     ]
    }
   ],
   "source": [
    "# Print the size of the DataFrame\n",
    "dataframe_size_mb = final_df.memory_usage(index=False, deep=True).sum() / (1024 * 1024)\n",
    "print(f\"Size of DataFrame: {dataframe_size_mb:.2f} MB\")\n",
    "\n",
    "# Print the size of the Parquet file\n",
    "parquet_size_mb = os.path.getsize(parquet_file_path) / (1024 * 1024)\n",
    "print(f\"Size of Parquet File on OS: {parquet_size_mb:.2f} MB\")\n",
    "\n",
    "# Print the difference between the two sizes\n",
    "size_diff = dataframe_size_mb - parquet_size_mb\n",
    "print(f\"Difference (DataFrame - Parquet): {size_diff:.2f} MB\")\n",
    "\n",
    "# Calculate and print the percentage difference\n",
    "percentage_diff = (size_diff / dataframe_size_mb) * 100\n",
    "print(f\"Percentage Difference (The Parquet as a percentage of the dataframe): {100 - percentage_diff:.2f}%\")\n",
    "\n",
    "\n",
    "print(f\"\\nDataFrame saved to {parquet_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Parquet file has 1209600 rows.\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Parquet file path\n",
    "parquet_file_path = 'output_data.parquet'\n",
    "\n",
    "# Read the Parquet file\n",
    "parquet_table = pq.read_table(parquet_file_path)\n",
    "\n",
    "# Get the number of rows\n",
    "num_rows = parquet_table.num_rows\n",
    "\n",
    "# Print the number of rows\n",
    "print(f\"The Parquet file has {num_rows} rows.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series opetations. \n",
    "Notice the file of the file (4 measures, history of 14 days, every 5 min ) is less than 4 MB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   hour  max_rows_read  avg_rows_read\n",
      "0   2023-10-01 10:00:00         999771  506988.653333\n",
      "1   2023-10-01 11:00:00         999453  502140.496944\n",
      "2   2023-10-01 12:00:00         999925  503711.897778\n",
      "3   2023-10-01 13:00:00         999671  494887.029444\n",
      "4   2023-10-01 14:00:00         999820  508576.183333\n",
      "..                  ...            ...            ...\n",
      "331 2023-10-15 05:00:00         999645  494570.926944\n",
      "332 2023-10-15 06:00:00         999632  495643.292778\n",
      "333 2023-10-15 07:00:00         999983  502477.309444\n",
      "334 2023-10-15 08:00:00         999800  494690.482222\n",
      "335 2023-10-15 09:00:00         999924  500413.425833\n",
      "\n",
      "[336 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Parquet file path\n",
    "parquet_file_path = 'output_data.parquet'\n",
    "\n",
    "# Read the Parquet file\n",
    "parquet_table = pq.read_table(parquet_file_path)\n",
    "\n",
    "# Convert the Parquet table to a DataFrame\n",
    "df = parquet_table.to_pandas()\n",
    "\n",
    "# Convert 'insert_date' to datetime type\n",
    "df['insert_date'] = pd.to_datetime(df['insert_date'])\n",
    "\n",
    "# Group by hourly time buckets and calculate max and average of 'rows_read'\n",
    "result_df = df.groupby(pd.Grouper(key='insert_date', freq='H')).agg({\n",
    "    'rows_read': ['max', 'mean']\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "result_df.columns = ['hour', 'max_rows_read', 'avg_rows_read']\n",
    "\n",
    "# Print the result\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 50 tables. By all calls in the last 14 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    table_name   rows_read\n",
      "153    tbl_237  2057343061\n",
      "229     tbl_35  2054438536\n",
      "286     tbl_87  2054421418\n",
      "224    tbl_300  2054414780\n",
      "227     tbl_33  2050699314\n",
      "49     tbl_143  2050192836\n",
      "68     tbl_160  2049931919\n",
      "90     tbl_180  2049427568\n",
      "54     tbl_148  2049276564\n",
      "61     tbl_154  2048014954\n",
      "202    tbl_281  2047625479\n",
      "46     tbl_140  2047230643\n",
      "136    tbl_221  2047160282\n",
      "83     tbl_174  2046569141\n",
      "279     tbl_80  2045802873\n",
      "88     tbl_179  2045543392\n",
      "270     tbl_72  2044999540\n",
      "201    tbl_280  2044513582\n",
      "166    tbl_249  2043855216\n",
      "28     tbl_124  2043787561\n",
      "278      tbl_8  2043016716\n",
      "167     tbl_25  2042748725\n",
      "291     tbl_91  2042600033\n",
      "298     tbl_98  2042143885\n",
      "175    tbl_257  2040451163\n",
      "32     tbl_128  2040036556\n",
      "2      tbl_100  2039972571\n",
      "268     tbl_70  2039928367\n",
      "79     tbl_170  2039816724\n",
      "281     tbl_82  2039458511\n",
      "97     tbl_187  2039446655\n",
      "170    tbl_252  2039285054\n",
      "16     tbl_113  2039213291\n",
      "271     tbl_73  2039142650\n",
      "139    tbl_224  2038371555\n",
      "187    tbl_268  2038340125\n",
      "253     tbl_57  2038250702\n",
      "58     tbl_151  2038160684\n",
      "52     tbl_146  2037480707\n",
      "96     tbl_186  2036032540\n",
      "106    tbl_195  2035850274\n",
      "87     tbl_178  2035599093\n",
      "75     tbl_167  2035269928\n",
      "6      tbl_104  2035211148\n",
      "8      tbl_106  2034964236\n",
      "162    tbl_245  2034723780\n",
      "98     tbl_188  2034609202\n",
      "237     tbl_42  2033636124\n",
      "134     tbl_22  2033353766\n",
      "38     tbl_133  2033222179\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "\n",
    "# Parquet file path\n",
    "parquet_file_path = 'output_data.parquet'\n",
    "\n",
    "# Read the Parquet file\n",
    "parquet_table = pq.read_table(parquet_file_path)\n",
    "\n",
    "# Convert the Parquet table to a DataFrame\n",
    "df = parquet_table.to_pandas()\n",
    "\n",
    "# Group by 'table_name' and calculate the total rows read for each table\n",
    "table_stats = df.groupby('table_name')['rows_read'].sum().reset_index()\n",
    "\n",
    "# Sort the DataFrame by total rows read in descending order and get the top 50 tables\n",
    "top_50_tables = table_stats.sort_values(by='rows_read', ascending=False).head(50)\n",
    "\n",
    "# Print the result\n",
    "print(top_50_tables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's collect Table Actitivy of every minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame:\n",
      "        host_name db_name table_name  rows_read  rows_inserted  rows_deleted  \\\n",
      "0          host_1    db_1      tbl_1     689469            969           626   \n",
      "1          host_1    db_1      tbl_2     466259            564           503   \n",
      "2          host_1    db_1      tbl_3     287934            282           451   \n",
      "3          host_1    db_1      tbl_4     592246            529           862   \n",
      "4          host_1    db_1      tbl_5     437674            378           783   \n",
      "...           ...     ...        ...        ...            ...           ...   \n",
      "6047995    host_1    db_1    tbl_296     836866            341           726   \n",
      "6047996    host_1    db_1    tbl_297     290389            354           198   \n",
      "6047997    host_1    db_1    tbl_298     164222            150           400   \n",
      "6047998    host_1    db_1    tbl_299     386835            611           852   \n",
      "6047999    host_1    db_1    tbl_300     429933            865            48   \n",
      "\n",
      "         rows_updated         insert_date  \n",
      "0                 237 2023-10-01 10:00:00  \n",
      "1                 574 2023-10-01 10:00:00  \n",
      "2                 459 2023-10-01 10:00:00  \n",
      "3                 228 2023-10-01 10:00:00  \n",
      "4                 381 2023-10-01 10:00:00  \n",
      "...               ...                 ...  \n",
      "6047995           594 2023-10-15 09:59:00  \n",
      "6047996           883 2023-10-15 09:59:00  \n",
      "6047997           577 2023-10-15 09:59:00  \n",
      "6047998           766 2023-10-15 09:59:00  \n",
      "6047999           522 2023-10-15 09:59:00  \n",
      "\n",
      "[6048000 rows x 8 columns]\n",
      "\n",
      "DataFrame saved to output_data_every_1_min.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Function to generate random data with constant host name, database, and timestamp\n",
    "def generate_batch_data(batch_size, batch_number, base_insert_date):\n",
    "    data = {\n",
    "        'host_name': ['host_1'] * batch_size,\n",
    "        'db_name': ['db_1'] * batch_size,\n",
    "        'table_name': [f'tbl_{i + 1}' for i in range(batch_size)],\n",
    "        'rows_read': [random.randint(1, 1000000) for _ in range(batch_size)],\n",
    "        'rows_inserted': [random.randint(1, 1000) for _ in range(batch_size)],\n",
    "        'rows_deleted': [random.randint(1, 1000) for _ in range(batch_size)],\n",
    "        'rows_updated': [random.randint(1, 1000) for _ in range(batch_size)],\n",
    "        'insert_date': [base_insert_date + timedelta(minutes=1 * batch_number) for _ in range(batch_size)],\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Number of batches and batch size\n",
    "num_batches = 60 * 24 * 14\n",
    "batch_size = 300\n",
    "\n",
    "# Base insert date for the first batch\n",
    "base_insert_date = datetime(2023, 10, 1, 10, 0, 0)\n",
    "\n",
    "# List to store DataFrames for each batch\n",
    "dfs = []\n",
    "\n",
    "# Generate and insert data in batches\n",
    "for i in range(num_batches):\n",
    "    df = generate_batch_data(batch_size, i, base_insert_date)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all batches into a single DataFrame\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(\"DataFrame:\")\n",
    "print(final_df)\n",
    "\n",
    "# Save the DataFrame to a Parquet file\n",
    "parquet_file_path = 'output_data_every_1_min.parquet'\n",
    "final_df.to_parquet(parquet_file_path, index=False)\n",
    "\n",
    "print(f\"\\nDataFrame saved to {parquet_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Parquet file has 6048000 rows.\n",
      "Size of DataFrame: 1312.99 MB\n",
      "Size of Parquet File on OS: 51.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Read the Parquet file\n",
    "parquet_table = pq.read_table(parquet_file_path)\n",
    "\n",
    "# Get the number of rows\n",
    "num_rows = parquet_table.num_rows\n",
    "\n",
    "# Print the number of rows\n",
    "print(f\"The Parquet file has {num_rows} rows.\")\n",
    "\n",
    "dataframe_size_mb = final_df.memory_usage(index=False, deep=True).sum() / (1024 * 1024)\n",
    "print(f\"Size of DataFrame: {dataframe_size_mb:.2f} MB\")\n",
    "\n",
    "# Print the size of the Parquet file\n",
    "parquet_size_mb = os.path.getsize(parquet_file_path) / (1024 * 1024)\n",
    "print(f\"Size of Parquet File on OS: {parquet_size_mb:.2f} MB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    table_name    rows_read\n",
      "214    tbl_292  10211198740\n",
      "128    tbl_214  10205936880\n",
      "239     tbl_44  10204089631\n",
      "184    tbl_265  10180037767\n",
      "127    tbl_213  10167842089\n",
      "7      tbl_105  10167093018\n",
      "213    tbl_291  10164856797\n",
      "89      tbl_18  10164282049\n",
      "183    tbl_264  10164037767\n",
      "157    tbl_240  10159929995\n",
      "164    tbl_247  10159683318\n",
      "2      tbl_100  10154205927\n",
      "131    tbl_217  10153732106\n",
      "295     tbl_95  10149615968\n",
      "271     tbl_73  10149068780\n",
      "17     tbl_114  10148302939\n",
      "134     tbl_22  10148006943\n",
      "146    tbl_230  10147698301\n",
      "100     tbl_19  10147022852\n",
      "159    tbl_242  10146966347\n",
      "144    tbl_229  10144879760\n",
      "152    tbl_236  10144260592\n",
      "186    tbl_267  10141064367\n",
      "260     tbl_63  10140807594\n",
      "284     tbl_85  10140282955\n",
      "21     tbl_118  10140061665\n",
      "251     tbl_55  10138525559\n",
      "182    tbl_263  10136552192\n",
      "96     tbl_186  10135036549\n",
      "242     tbl_47  10132872987\n",
      "153    tbl_237  10132229004\n",
      "249     tbl_53  10132157773\n",
      "254     tbl_58  10130984045\n",
      "59     tbl_152  10129561663\n",
      "138    tbl_223  10129437028\n",
      "93     tbl_183  10128935023\n",
      "187    tbl_268  10128882125\n",
      "130    tbl_216  10128688291\n",
      "5      tbl_103  10128005767\n",
      "91     tbl_181  10127840457\n",
      "106    tbl_195  10127413371\n",
      "72     tbl_164  10126754126\n",
      "168    tbl_250  10126562446\n",
      "217    tbl_295  10125719749\n",
      "243     tbl_48  10125435701\n",
      "45      tbl_14  10124894250\n",
      "28     tbl_124  10123592529\n",
      "218    tbl_296  10123089706\n",
      "50     tbl_144  10122446225\n",
      "19     tbl_116  10122358288\n"
     ]
    }
   ],
   "source": [
    "# Convert the Parquet table to a DataFrame\n",
    "df = parquet_table.to_pandas()\n",
    "\n",
    "# Group by 'table_name' and calculate the total rows read for each table\n",
    "table_stats = df.groupby('table_name')['rows_read'].sum().reset_index()\n",
    "\n",
    "# Sort the DataFrame by total rows read in descending order and get the top 50 tables\n",
    "top_50_tables = table_stats.sort_values(by='rows_read', ascending=False).head(50)\n",
    "\n",
    "# Print the result\n",
    "print(top_50_tables)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
