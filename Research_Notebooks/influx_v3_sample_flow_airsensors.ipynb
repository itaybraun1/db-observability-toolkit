{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "\n",
    "<body>\n",
    "    <h2><span style=\"color: lightblue;\">Influx v3</span></h2>\n",
    "    <h3><strong>Desc:</strong></h3>\n",
    "    <ul>\n",
    "        <li>Write metrics to Influx Database.</li>\n",
    "        <li>Read Metrics from a bucket.</li>\n",
    "    </ul>\n",
    "    <h3>Comments:</h3>\n",
    "    <ul>\n",
    "        <li>This notebook is still in <strong>DRAFT</strong> mode.</li>\n",
    "        <li>This notebook was developed for Influx cloud v3. Do NOT use it with v2</li>\n",
    "    </ul>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Req\n",
    "1. Open an Iflux DB cloud account. Can be the free one. https://www.influxdata.com/get-influxdb/    \n",
    "2. Create a new Bucket. For ex. ```Tables_bucket```.   \n",
    "3. To learn about the InfluxDB terminology, rad here: https://docs.influxdata.com/influxdb/v2/get-started/ \n",
    "   \n",
    "The rest of the flow follows the built-in samples\n",
    "1. Open the \"Load Data\" option of \"Client\" \n",
    "2. Select \"Python\"\n",
    "3. Follow the instructions. Step \"install dependencies\" is pip install influxdb3-python, pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Token. \n",
    "Without a token, the client code can't call the API. To get a Token, log in to the cloud console, Load Data, API Tokens, Generate API Token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export INFLUXDB_TOKEN=XE9AyZ-3y-HJNyupKWiLgzVo5JMew-Y31Vq7gbakekdP66wIkBslEdnyrCc-vQ0t9MGFj449z0LvFhepVOwFfw=="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Client\n",
    "Copy the code. I named the organization in Influx \"Dev\"\n",
    "\n",
    "Notice. The original code, proivded by Influx returns an error: ```[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)``` . To solve it, add the coomand ``` ssl_ca_cert=certifi.where()```. Resource: # https://stackoverflow.com/questions/69401104/influxdb-2-0-certificate-verify-failed-certificate-has-expired-ssl-c1129\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "from influxdb_client_3 import InfluxDBClient3, Point\n",
    "import certifi # we need it to support their certification problems. \n",
    "\n",
    "# Read from the OS or pass the parameter directly. \n",
    "token = os.environ.get(\"INFLUXDB_TOKEN\")\n",
    "token = \"XE9AyZ-3y-HJNyupKWiLgzVo5JMew-Y31Vq7gbakekdP66wIkBslEdnyrCc-vQ0t9MGFj449z0LvFhepVOwFfw==\"\n",
    "print (token)\n",
    "org = \"Dev\"\n",
    "host = \"https://us-east-1-1.aws.cloud2.influxdata.com\"\n",
    "\n",
    "client = InfluxDBClient3(host=host, token=token, org=org, ssl_ca_cert=certifi.where())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy (Insert) Data  \n",
    "The object \"Bucket\" in the UI called \"database\" in python.\n",
    "The measurement called \"Census\". Notic the ```Point``` object uses it.  \n",
    "\n",
    "\n",
    "\n",
    "In this data example, we have some important concepts:\n",
    "- **measurement**: Primary filter for the thing you are measuring. Since we are measuring the sample census of insects, our measurement is \"census\".\n",
    "- **tag**: Key-value pair to store metadata about your fields. We are storing the \"location\" of where each census is taken. Tags form part of your primary key.\n",
    "- **field**:\tKey-value pair that stores the actual data you are measuring.\tWe are storing the insect \"species\" and \"count\" as the key-value pair. Fields are not indexed and can be stored as integers, floats, strings, or booleans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database=\"Tables_Bucket\"\n",
    "\n",
    "data = {\n",
    "  \"point1\": {\n",
    "    \"location\": \"Klamath\",\n",
    "    \"species\": \"bees\",\n",
    "    \"count\": 25,\n",
    "  },\n",
    "  \"point2\": {\n",
    "    \"location\": \"Portland\",\n",
    "    \"species\": \"ants\",\n",
    "    \"count\": 32,\n",
    "  },\n",
    "  \"point3\": {\n",
    "    \"location\": \"Klamath\",\n",
    "    \"species\": \"bees\",\n",
    "    \"count\": 28,\n",
    "  },\n",
    "  \"point4\": {\n",
    "    \"location\": \"Portland\",\n",
    "    \"species\": \"ants\",\n",
    "    \"count\": 36,\n",
    "  },\n",
    "  \"point5\": {\n",
    "    \"location\": \"Klamath\",\n",
    "    \"species\": \"bees\",\n",
    "    \"count\": 27,\n",
    "  },\n",
    "  \"point6\": {\n",
    "    \"location\": \"Portland\",\n",
    "    \"species\": \"ants\",\n",
    "    \"count\": 43,\n",
    "  },\n",
    "}\n",
    "\n",
    "for key in data:\n",
    "  point = (\n",
    "    Point(\"census\")\n",
    "    .tag(\"location\", data[key][\"location\"])\n",
    "    .field(data[key][\"species\"], data[key][\"count\"])\n",
    "  )\n",
    "  client.write(database=database, record=point)\n",
    "  time.sleep(1) # separate points by 1 second\n",
    "\n",
    "print(\"Complete. Return to the InfluxDB UI.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert data from a CSV\n",
    "The CLI Code uses a CSV stored in S3: https://influx-testdata.s3.amazonaws.com/air-sensor-data-annotated.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# influx write --bucket Tables_Bucket --url https://influx-testdata.s3.amazonaws.com/air-sensor-data-annotated.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute a Simple Query - SQL \n",
    "v3 supports SQL. v2 couldn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT *\n",
    "FROM 'census'\n",
    "WHERE time >= now() - interval '168 hours'\n",
    "AND ('bees' IS NOT NULL OR 'ants' IS NOT NULL)\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "table = client.query(query=query, database=\"Tables_Bucket\", language='sql') \n",
    "\n",
    "# Convert to dataframe\n",
    "df = table.to_pandas().sort_values(by=\"time\")\n",
    "column_names = df.columns\n",
    "print(column_names)\n",
    "print(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advance Query - SQL \n",
    "Sep 17 - doesn't work. Not sure why as it is copy paste from the demo. \n",
    "I suspect the fact I had to use \"sql\" as a language to make it work, and not \"influxql\" is the root cause. The documentation says using ```import influxdb_client_3 as InfluxDBClient3``` with influxql but it doesn't work.    \n",
    "\n",
    "Trying to implement ```GROUP BY``` using: https://docs.influxdata.com/influxdb/v1/query_language/explore-data/#the-group-by-clause "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Execute Aggregate Queries. The first one works\n",
    "query = \"\"\"\n",
    "SELECT  location, max(time), avg(census.ants)\n",
    "FROM \"census\"\n",
    "WHERE time >= now() - interval '1 hour'\n",
    "AND (ants IS NOT NULL)\n",
    "GROUP BY location\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Execute the query\n",
    "table = client.query(query=query, database=\"Tables_Bucket\", language='sql') \n",
    "\n",
    "# Convert to dataframe\n",
    "df = table.to_pandas()#.sort_values(by='time')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the data - Group by time ranges\n",
    "You can always use a good olf Flux to run a GROUP BY query. Group the data every 5 min. \n",
    "It the query doesn't return any data, that means that no data was inserted to this Influx Bucket. See one of the cells above how to insert data.   \n",
    "\n",
    "- Example 1 - running using the Influx CLI ( ```brew install influxdb-cli```). Apparently it still works with Flux. \n",
    "- Exampel 2 - Running using Python. It can't use Flux anymore. Only InfluQL ( https://docs.influxdata.com/influxdb/v1/query_language/, supported languages now are only SQL or InfluxQL: https://docs.influxdata.com/influxdb/cloud-dedicated/reference/client-libraries/v3/python/#functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "influx query \\\n",
    "'from(bucket: \"Tables_Bucket\")\n",
    "  |> range(start: -1h)\n",
    "  |> filter(fn: (r) => r._measurement == \"airSensors\" and r._field == \"humidity\")\n",
    "  |> aggregateWindow(every: 15m, fn: max)'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource: Query Data with InfluxQL: https://docs.influxdata.com/influxdb/cloud/query-data/influxql/\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from influxdb_client import InfluxDBClient, Point\n",
    "from datetime import timedelta\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Define your InfluxDB connection details\n",
    "#url = \"http://localhost:8086\"\n",
    "#token = \"your_influxdb_token\"\n",
    "#org = \"your_organization\"\n",
    "bucket = \"Tables_Bucket\"\n",
    "\n",
    "# Create an InfluxDB client instance\n",
    "#client = InfluxDBClient(url=url, token=token, org=org)\n",
    "\n",
    "# SQL Query (NOT Flux)\n",
    "query = \"\"\"\n",
    "SELECT max(humidity)\n",
    "FROM airSensors\n",
    "WHERE time >= '2023-09-25T00:00:00Z'\n",
    "GROUP BY time(60m)\n",
    "\"\"\"\n",
    "\n",
    "token = os.environ.get(\"INFLUXDB_TOKEN\")\n",
    "token = \"XE9AyZ-3y-HJNyupKWiLgzVo5JMew-Y31Vq7gbakekdP66wIkBslEdnyrCc-vQ0t9MGFj449z0LvFhepVOwFfw==\"\n",
    "org = \"Dev\"\n",
    "host = \"https://us-east-1-1.aws.cloud2.influxdata.com\"\n",
    "\n",
    "client = InfluxDBClient3(host=host, token=token, org=org, ssl_ca_cert=certifi.where())\n",
    "# You can bring only the schema to help troubleshooting\n",
    "# schema = client.query(query=query, database=\"Tables_Bucket\", mode=\"schema\", language=\"influxql\")\n",
    "# print(schema)\n",
    "\n",
    "table = client.query(query=query, database=\"Tables_Bucket\", mode =\"all\", language=\"influxql\")\n",
    "dataframe = table.to_pandas() # This one automatically eliminitaes the NULL values. Not good. \n",
    "print (table)\n",
    "\n",
    "# Create a dictionary from the data\n",
    "data_dict = {\n",
    "    \"Measurement\": table[0],\n",
    "    \"Time\": table[1],\n",
    "    \"Max\": table[2]\n",
    "}\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame(data_dict)\n",
    "\n",
    "print(df)\n",
    "\n",
    "# Create a line plot using Plotly Express\n",
    "fig = px.line(df, x=\"Time\", y=\"Max\", title=\"Max Values Over Time\", labels={\"Max\": \"Max Value\"})\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Close the client connection\n",
    "client.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on V3 - real world data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resource: Query Data with InfluxQL: https://docs.influxdata.com/influxdb/cloud/query-data/influxql/\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from influxdb_client import InfluxDBClient, Point\n",
    "from datetime import timedelta\n",
    "\n",
    "# token = \"ylThfbJSW4MDfbcEIyNQsS2vnHOBjz-Uanc0KgHAZIcpBSRPc1WaPRwVKmdNzPj8H-HNQyEaN32dqrC10zjVWg==\"\n",
    "token = \"PjObG70ZggAu78U5hN9awHq5pOk6GOtAsu7Fp4JROzdHmllRCMrlQh0r9owylgKR0A_Ki7-EYgzz03TNad0tqw==\"\n",
    "org = \"Metis-v3\"\n",
    "host = \"https://eu-central-1-1.aws.cloud2.influxdata.com\"\n",
    "bucket = \"test\"\n",
    "\n",
    "client = InfluxDBClient3(host=host, token=token, org=org, ssl_ca_cert=certifi.where())\n",
    "# You can bring only the schema to help troubleshooting\n",
    "# schema = client.query(query=query, database=\"Tables_Bucket\", mode=\"schema\", language=\"influxql\")\n",
    "# print(schema)\n",
    "\n",
    "# InfluxQL Query (NOT Flux)\n",
    "# The last 14 days of a specific query ID, group by hour. \n",
    "\n",
    "### THIS QUERY DOESN'T WORK - the query should show the hourly diff of every hour. It shows 0.\n",
    "query = \"\"\"\n",
    "SELECT time, \n",
    "    max(calls)\n",
    "    --DERIVATIVE(max(calls))\n",
    "FROM QUERY_DETAILS\n",
    "WHERE time >= '2023-09-14T00:00:00Z'\n",
    "AND apiKey = 'mj1Vde9QC664608cJ24CV3Zf2Y9tdgzt20P8quOm'\n",
    "AND host = 'database-2.cofhrj7zmyn4.eu-central-1.rds.amazonaws.com'\n",
    "AND db = 'platform-v2'\n",
    "AND query_id = '-1004574566510211833'\n",
    "GROUP BY time(60m)\n",
    "\"\"\"\n",
    "table = client.query(query=query, database=bucket, mode =\"all\", language=\"influxql\")\n",
    "dataframe = table.to_pandas() # This one automatically eliminitaes the NULL values. Not good. \n",
    "# print (table)\n",
    "\n",
    "# Create a dictionary from the data\n",
    "data_dict = {\n",
    "    \"Measurement\": table[0],\n",
    "    \"Time\": table[1],\n",
    "    \"Max\": table[2]\n",
    "}\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame(data_dict)\n",
    "print(df)\n",
    "\n",
    "\n",
    " ############################################\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "  SUM(\"calls\") AS sum_calls\n",
    "FROM\n",
    "  \"test\".\"autogen\".\"QUERY_DETAILS\"\n",
    "WHERE\n",
    "  time >= now() - 2h\n",
    "  AND apiKey = 'mj1Vde9QC664608cJ24CV3Zf2Y9tdgzt20P8quOm'\n",
    "  AND \"host\" = 'database-2.cofhrj7zmyn4.eu-central-1.rds.amazonaws.com'\n",
    "  AND \"db\" = 'platform-v2'\n",
    "GROUP BY\n",
    "  time(1h), apiKey, \"host\", \"db\", \"query_id\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "table = client.query(query=query, database=bucket, mode =\"all\", language=\"influxql\")\n",
    "dataframe = table.to_pandas() # This one automatically eliminitaes the NULL values. Not good. \n",
    "print\n",
    "\n",
    "data_dict = {\n",
    "    \"Measurement\": table[0],\n",
    "    \"Time\": table[1],\n",
    "    \"apiKey\":table[2],\n",
    "    \"Max\": table[6]\n",
    "}\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame(data_dict)\n",
    "df\n",
    "\n",
    "plot_table = ff.create_table(df)\n",
    "\n",
    "# Display the table in Jupyter Notebook\n",
    "plot_table.show()\n",
    "\n",
    "# Close the client connection\n",
    "client.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Line Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "influx write -b Tables_Bucket -o Dev -f /Users/itaybraun/Documents/GitHub/db-observability-toolkit/Research_Notebooks/influx_line_protocol_1.txt\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "influx write --bucket Tables_Bucket --f /Users/itaybraun/Documents/GitHub/db-observability-toolkit/Research_Notebooks/influx_csv.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing CSV, now with more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define the parameters\n",
    "host = \"host1\"\n",
    "database = \"db1\"\n",
    "query_ids = [\"qry1\", \"qry2\", \"qry3\", \"qry4\", \"qry5\", \"qry6\", \"qry7\", \"qry8\", \"qry9\", \"qry10\"]\n",
    "start_time = datetime(2023, 9, 13, 0, 0, 0)  # Sep 13, 2023, 00:00:00\n",
    "calls_increment = 1000\n",
    "time_difference = timedelta(minutes=5)\n",
    "\n",
    "# Generate and write the CSV data\n",
    "with open('queries.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['#datatype measurement', 'tag', 'tag', 'tag', 'double', 'double', 'double', 'dateTime:RFC3339']\n",
    "    writer = csv.writer(csvfile)\n",
    "    \n",
    "    # Write the first row of metadata\n",
    "    writer.writerow(fieldnames)\n",
    "\n",
    "    # Write the second row with field names\n",
    "    writer.writerow(['queries', 'host', 'db', 'query_id', 'calls', 'total_exec_time', 'rows_read', 'time'])\n",
    "\n",
    "    # Generate 400 rows for each of the 10 query IDs\n",
    "    for query_id in query_ids:\n",
    "        for i in range(4000):\n",
    "            timestamp = start_time + i * time_difference\n",
    "            calls = i * calls_increment\n",
    "\n",
    "            # Create a list representing a row of data\n",
    "            row = [\n",
    "                'queries',\n",
    "                host,\n",
    "                database,\n",
    "                query_id,\n",
    "                calls,\n",
    "                calls * 8,  # Assuming a constant multiplier for total_exec_time\n",
    "                calls * 1000,  # Assuming a constant multiplier for rows_read\n",
    "                timestamp.isoformat() + 'Z'\n",
    "            ]\n",
    "\n",
    "            # Write the row to the CSV file\n",
    "            writer.writerow(row)\n",
    "\n",
    "print(\"Data generation complete. Output written to queries.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the file was created succesfully. and its size. \n",
    "import os\n",
    "\n",
    "file_path = 'queries.csv'\n",
    "\n",
    "# Function to get the number of rows and file size\n",
    "def get_file_stats(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Read the number of rows (excluding header rows)\n",
    "        num_rows = sum(1 for row in file) - 2  # Subtracting 2 for the header rows\n",
    "\n",
    "    # Get file size in kilobytes\n",
    "    file_size_kb = os.path.getsize(file_path) / 1024\n",
    "\n",
    "    return num_rows, file_size_kb\n",
    "\n",
    "# Get file statistics\n",
    "num_rows, file_size_kb = get_file_stats(file_path)\n",
    "\n",
    "# Format the number of rows with commas\n",
    "formatted_num_rows = \"{:,}\".format(num_rows)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Number of rows in the file: {formatted_num_rows}\")\n",
    "print(f\"File size: {file_size_kb:.2f} KB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# influx write -b Tables_Bucket -o Dev -f /Users/itaybraun/Documents/GitHub/db-observability-toolkit/Research_Notebooks/queries.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read using InfluxQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "influx query \\\n",
    "'from(bucket: \"Tables_Bucket\")\n",
    "  |> range(start: -14d, stop: -12d)\n",
    "  |> filter(fn: (r) => r._measurement == \"m\" and r._field == \"calls\" and r.query_id == \"qry1\")\n",
    "  |> aggregateWindow(every: 60m, fn: max)\n",
    "  |> difference(nonNegative: true)\n",
    "  |> drop(columns: [\"_start\", \"_stop\", \"_measurement\"])'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your InfluxDB connection details\n",
    "#url = \"http://localhost:8086\"\n",
    "#token = \"your_influxdb_token\"\n",
    "#org = \"your_organization\"\n",
    "bucket = \"Tables_Bucket\"\n",
    "\n",
    "# SQL Query (NOT Flux)\n",
    "query = \"\"\"\n",
    "SELECT query_id, max(calls)\n",
    "FROM \"queries\"\n",
    "WHERE time >= '2023-09-25T00:00:00Z'\n",
    "GROUP by query_id, time(60m)\n",
    "\"\"\"\n",
    "\n",
    "## Query_2 Doesn't work\n",
    "query_2 = \"\"\"\n",
    "SELECT derivative(max(\"calls\"), 5m) AS \"difference_calls\"\n",
    "FROM \"queries\"\n",
    "WHERE time >= -14d AND time < -12d AND query_id = 'qry1'\n",
    "GROUP BY time(60m) fill(null)\n",
    "\"\"\"\n",
    "\n",
    "token = os.environ.get(\"INFLUXDB_TOKEN\")\n",
    "token = \"XE9AyZ-3y-HJNyupKWiLgzVo5JMew-Y31Vq7gbakekdP66wIkBslEdnyrCc-vQ0t9MGFj449z0LvFhepVOwFfw==\"\n",
    "org = \"Dev\"\n",
    "host = \"https://us-east-1-1.aws.cloud2.influxdata.com\"\n",
    "\n",
    "client = InfluxDBClient3(host=host, token=token, org=org, ssl_ca_cert=certifi.where())\n",
    "# You can bring only the schema to help troubleshooting\n",
    "# schema = client.query(query=query, database=\"Tables_Bucket\", mode=\"schema\", language=\"influxql\")\n",
    "# print(schema)\n",
    "\n",
    "table = client.query(query=query_2, database=\"Tables_Bucket\", mode =\"all\", language=\"influxql\")\n",
    "dataframe = table.to_pandas() # This one automatically eliminitaes the NULL values. Not good. \n",
    "print (table)\n",
    "\n",
    "\n",
    "# Close the client connection\n",
    "client.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete a bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# InfluxDB API endpoint\n",
    "base_url = \"http://localhost:8086\"\n",
    "org = \"your_organization\"  # Replace with your organization name\n",
    "bucket = \"your_bucket\"      # Replace with the name of the bucket you want to delete\n",
    "\n",
    "# Authentication token (if required)\n",
    "token = \"your_authentication_token\"  # Replace with your authentication token, if needed\n",
    "\n",
    "# Construct the URL for deleting the bucket\n",
    "url = f\"{base_url}/api/v2/buckets/{org}/{bucket}\"\n",
    "\n",
    "# Headers for the request (include the authentication token if required)\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {token}\" if token else \"\",\n",
    "}\n",
    "\n",
    "# Send the DELETE request to delete the bucket\n",
    "response = requests.delete(url, headers=headers)\n",
    "\n",
    "# Check the response status code\n",
    "if response.status_code == 204:\n",
    "    print(f\"Bucket '{bucket}' deleted successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to delete bucket '{bucket}'. Status code: {response.status_code}\")\n",
    "    print(response.text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
